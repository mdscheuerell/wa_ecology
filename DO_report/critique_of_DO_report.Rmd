---
title: "Critique of model evaluation by WA Dept of Ecology"
author: "Mark Scheuerell"
date: "10/31/2019"
output:
  pdf_document:
    highlight: haddock
fontsize: 11pt
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "pdf")

if(!require("captioner")) {
  devtools::install_github("adletaw/captioner")
  library("captioner")
}

## set default caption delimter
fig_cap <- captioner(suffix = ".")
## initialize figure number
cnt <- 1
```


# Comparison of existing and reference scenarios

The focus of the modeling analysis is a comparison of results obtained with two scenarios: a "reference" case that represents a system without anthropogenic inputs, and an "existing" case that represents contemporary conditions. Specifically, Ecology is interested in the difference between the modeled concentration of dissolved oxygen estimated via the two models. In addition, Ecology would like to know the estimated uncertainty in that difference.

## Variance of predictions

In the section titled "Uncertainty in Dissolved Oxygen Depletion Estimates" (p59), it states,

> The RMSE of differences is calculated to understand the uncertainty associated with the result of subtracting one model scenario from another model scenario (i.e., the difference between two model scenarios). In this case, we calculated the error associated with the DO depletions computed from the difference between the existing and reference model scenarios.

The section then goes on to describe how the calculations were made using the estimated root mean squared error (RMSE) between the predictions and observations, but there is a mistake in the assumed relationship between the standard deviation of the predictions and the RMSE.

### An example

To demonstrate this, consider this simple equation that relates individual observations ($o_i$) and predictions ($p_i$):

$$
o_i = p_i + e_i,
$$

where $e_i$ are the model prediction errors (i.e., the difference between the observed and predicted values). From this relationship we know that the variance of the observations is a function of the variances of both the predictions and errors, and their covariance, such that

$$
\text{Var}(o) = \text{Var}(p) + \text{Var}(e) + 2 ~ \text{Cov}(p, e)
$$

In this case our interest of comparison is the uncertainty in the difference in predictions between two different models, not the uncertainty in the difference in residuals between the two models. Therefore, we can rewrite the above equation to show that the variance of the predictions is

$$
\text{Var}(p) = \text{Var}(o) - \text{Var}(e) + 2 ~ \text{Cov}(p, e).
$$

## Variance in the difference of predictions

In this case Ecology is interested in the uncertainty (variance) in the difference between the predictions from the two models representing existing and reference conditions, which we write as $p_{ex}$ and $p_{ref}$, respectively. We then define the difference $\delta$ as

$$
\delta = p_{ex} - p_{ref}
$$

and hence

$$
\begin{aligned}
\text{Var}(\delta) &= \text{Var}(p_{ex}) + \text{Var}(p_{ref}) - 2 ~ \text{Cov}(p_{ex}, p_{ref}) \\
&= \text{Var}(p_{ex}) + \text{Var}(p_{ref}) - 2 ~ \text{Cor}(p_{ex}, p_{ref}) \text{SD}(p_{ex}) \text{SD}(p_{ref})
\end{aligned}
$$

This is where Ecology gets their calculations wrong. In a forecasting context, the hope is that the predictions match the observations very closely and hence the errors are small. One measure of forecast skill is the root mean-squared error, which equals the standard deviation of the errors. More specifically,

$$
\text{RMSE}_{o,p} = \text{SD}(e) = \sqrt{\text{Var}(e)} = \sqrt{\frac{\sum (p_i - o_i)^2}{N}}.
$$

Importantly, however, the $\text{RMSE}_{o,p}$ is not equal to the variance of the predictions, $\text{Var}(p)$, the latter of which is required for the calculations of the error in differences.

### Re-analysis

The Ecology report does not provide estimates of the variance in the model predictions, but we can generate approximations from the information provided and a simple assumption. For most of the DO models, $\text{RMSE}_{ex} \approx 1$ (Table 7) and the correlation between the predicted and observed values is about 0.85 (Table 8). Recognizing that 

$$
\text{RMSE}_{ex} = \sqrt{(1 - R^2)} \text{SD}(o),
$$

we can estimate the SD of the observations as 

$$
\text{SD}(o) = \frac{\text{RMSE}_{ex}}{\sqrt{(1 - R^2)}} \approx \frac{1}{\sqrt{(1 - 0.85^2)}} \approx 1.9
$$

and hence the variance of the observations is

$$
\text{Var}(o) = \text{SD}(o)^2 \approx 1.9^2 = 3.61.
$$

Now we can estimate the variance of the predictions for the model with existing conditions as above, with

$$
\begin{aligned}
\text{Var}(p_{ex}) &= \text{Var}(o) - \text{Var}(e) + 2 ~ \text{Cov}(p_{ex}, e) \\
  &= \text{Var}(o) - \text{RMSE}_{ex}^2 + 2 ~ \text{Cov}(p_{ex}, e) \\
  &\approx 3.6 - 1 + 2 ~ \text{Cov}(p_{ex}, e).
\end{aligned}
$$

Absent information on the covariance between the predicted values and the model errors, we will assume that the model is well behaved and $\text{Cov}(p_{ex}, e) \approx 0$, such that

$$
\text{Var}(p_{ex}) \approx 3.6 - 1 + 2(0) = 2.6
$$

To the extent that $\text{Cov}(p_{ex}, e)$ is positive (negative), $text{Var}(p_{ex})$ will be larger (smaller) than this estimate.

If we also assume, as Ecology did, that $\text{Var}(p_{ex}) = \text{Var}(p_{ref})$, then we can estimate the variance in the difference ($\delta$) between the predictions from the two models as above, such that

$$
\begin{aligned}
\text{Var}(\delta) &= \text{Var}(p_{ex}) + \text{Var}(p_{ref}) - 2 ~ \text{Cor}(p_{ex}, p_{ref}) ~  \text{SD}(p_{ex}) ~  \text{SD}(p_{ref}) \\
  &= \text{Var}(p_{ex}) + \text{Var}(p_{ex}) - 2 ~ \text{Cor}(p_{ex}, p_{ref}) ~  \text{SD}(p_{ex})  ~ \text{SD}(p_{ex}) \\
  &= 2 ~ \text{Var}(p_{ex}) - 2 ~ \text{Cor}(p_{ex}, p_{ref}) ~ \text{Var}(p_{ex}) \\
  &= 2 ~ \text{Var}(p_{ex}) [1 - \text{Cor}(p_{ex}, p_{ref})] \\
  &= 2(2.6) [1 - \text{Cor}(p_{ex}, p_{ref})].
\end{aligned}
$$

Thus, if $\text{Cor}(p_{ex}, p_{ref}) = 0$, then $\text{Var}(\delta) = 5.2 \Rightarrow \text{SD}(\delta) \approx 2.3$; conversely, as $\text{Cor}(p_{ex}, p_{ref}) \rightarrow 1 \Rightarrow \text{Var}(\delta) \rightarrow 0$.

# An example

Here is a simple example that shows how $\text{SD}(\hat{y})$ and $\text{RMSE}(\hat{y})$ are different. Consider a case where we had reason to believe that a variable $y$ was a function of another variable $x$. In effort to undercover the nature of their relationship, we collected 20 samples of both $y$ and $x$ (Figure 1).

```{r fig_1_data, echo = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
## set random seed for reprodicibility
set.seed(123)

## function to calculate RMSE
rmse <- function(pre, obs) {
  rmse <- sqrt(sum((pre - obs)^2)/length(pre))
  return(rmse)
}

## sample size
nn <- 20
## some values for the predictor x
xx <- sort(runif(nn, 0, 20))
## intercept
beta_0 <- 1
## slope
beta_1 <- 0.5
## random errors (mean = 0, SD = 1)
epsilon <- rnorm(nn)
## observed data
yy <- beta_0 + beta_1*xx + epsilon

## fit a regression model
mm <- lm(yy ~ xx)
## get predicted values
yhat <- predict(mm)

par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1,
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Plot of some hypothetical data.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}

Based on the apparent relationship between $x$ and $y$, we might assume that each of the observed values $y_i$ is a linear combination of an intercept $\beta_0$, the effect $\beta_1$ of a covariate $x_i$, and some random observation error $\epsilon_i$, such that

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i,
$$

and $\epsilon_i \sim \text{N}(0, \sigma)$. We could easily estimate the unknown parameters in this model ($\beta_0$, $\beta_1$, $\sigma$), and then use the deterministic portion of the model to make predictions to compare with each of the observed values. Specifically, the predictions $(\hat{y}_i)$ would be given by a straight line, such that

$$
\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i.
$$

We could then estimate the SD and RMSE of these predictions (Figure 2). It turns out that the SD of $\hat{y}$ is ~`r round(sd(yhat), 2)`, but the RMSE is only ~`r round(rmse(yhat, yy), 2)`, which is about `r round(sd(yhat) / rmse(yhat, yy), 0)` times less.


```{r fig_2_plot_regr_ex, echo = FALSE, fig.width = 8, fig.height = 4.5, fig.align="center"}
par(mfrow = c(1,2), mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 16, las = 1, type = "n",
     xlab = expression(italic(x)),
     ylab = expression(hat(italic(y))~~or~~m[hat(italic(y))]))
mtext("SD", side = 3, line = 1)
segments(x0 = xx, y0 = yhat, y1 = mean(yhat), col = "gray")
points(xx, yhat, pch = 21, bg = "white")
points(xx, rep(mean(yy), nn), pch = 19)
## plot observed and yhat
plot(xx, yy, pch = 16, las = 1, type = "n",
     xlab = expression(italic(x)),
     ylab = expression(italic(y)~~or~~hat(italic(y))))
mtext("RMSE", side = 3, line = 1)
segments(x0 = xx, y0 = yhat, y1 = yy, col = "gray")
points(xx, yy, pch = 19)
points(xx, yhat, pch = 21, bg = "white")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Graphical examples of the difference between the SD of the predictions (left) and the RMSE of the predictions (right). For the SD, the comparison is based upon the differences between the predictions (open circles) and their mean (filled circles). For the RMSE, the comparison is based upon differences between the predictions (open circles) and the observed data (filled circles). In both cases, one would square the length of each of the vertical gray lines, sum them up, and divide by the number of them before finally taking the square root.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.25in}

# Prediction errors

The above example dismisses an important aspect of RMSE: it should be used to compare "out of sample" predictions. Furthermore, RMSE give us an indication as to the predictive error, *on average*, rather than the uncertainty in a specific prediction.

Returning to our example above, we could estimate our uncertainty around the fitted relationship between $x$ and $y$ with a confidence interval (CI), which would give us an indication of the range of where the "true" fitted values would lie had we repeated our sampling exercise many times. Specifically, a $(1 - \alpha)100$% CI on the expected relationship between $x$ and $y$ at some value $x_k$ is given by

$$
\hat{y}_{i} \pm t_{\alpha / 2, n-2} \sqrt{\sigma \left(\frac{1}{n}+\frac{\left(x_{k}-\bar{x}\right)^{2}}{\sum\left(x_{i}-\bar{x}\right)^{2}}\right)}.
$$

The interval increases as the distance between $x_k$ and $\bar{x}$ increases (Figure 3).

```{r fig_3_CI, echo = FALSE, warning = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
y_ci <- predict(mm, interval = "confidence")
y_pi <- predict(mm, interval = "prediction")

par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1, ylim = range(y_pi),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
lines(xx, y_ci[,"fit"])
lines(xx, y_ci[,"lwr"], col = "gray")
lines(xx, y_ci[,"upr"], col = "gray")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Example of a 95% confidence interval (gray lines) around the expected relationship between $x$ and $y$ (black line).", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}

In a case like this, however, where we wish to make out-of-sample predictions about some new state of nature, our uncertainty around any single prediction will be necessarily greater. Specifically, a $(1 - \alpha)100$% prediction interval (PI) around $\hat{y}$ at some value $x_k$ is given by

$$
\hat{y} \pm t_{\alpha / 2, n-2} \sqrt{\sigma \left(1 + \frac{1}{n}+\frac{\left(x_{k}-\bar{x}\right)^{2}}{\sum\left(x_{i}-\bar{x}\right)^{2}}\right)}.
$$

```{r y_new, echo = FALSE}
y_new <- predict(mm, new = data.frame(xx = 10), interval = "prediction")
```

Here the paranthetic multiplier on the residual variance $\sigma$ has increased by 1, which means the prediction interval is wider (less certain) than the confidence interval (Figure 4). This is because the CI only needs to account for uncertainty in estimating the expected value of $y$ whereas the PI needs to account for a random future value of $y$ that tend to fall away from the mean.

```{r fig_4_PI, echo = FALSE, warning = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1, ylim = range(y_pi),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
lines(xx, y_pi[,"fit"])
lines(xx, y_pi[,"lwr"], col = "gray")
lines(xx, y_pi[,"upr"], col = "gray")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Example of a 95% prediction interval (gray lines) for future unobserved values of $y$.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}

So, for example, if we wanted to predict, with 95% certainty, what we would observe for $y$ if $x = 10$, we would get `r round(y_new[,"fit"], 2)` $\pm$ `r round(y_new[,"fit"] - y_new[,"lwr"], 2)` (Figure 5). The relatively wide prediction interval suggests that it might be difficult to discern the prediction for $y$ when $x = 10$ to the expected values for $y$ if $x$ were as low as 5 or as high as 15.

\vspace{0.2in}

```{r fig_5_new_y, echo = FALSE, warning = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1, ylim = range(y_pi),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
lines(xx, y_pi[,"fit"])
abline(h = y_new[,"lwr"], col = "gray")
abline(h = y_new[,"upr"], col = "gray")
segments(10, y0 = y_new[,"lwr"], y1 = y_new[,"upr"], col = "blue")
points(10, y_new[,"fit"], pch = 19, col = "blue")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Example of the uncertainty around a new prediction for $y$ when $x = 10$.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}




