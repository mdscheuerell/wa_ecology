---
title: "Critique of model evaluation by WA Dept of Ecology"
author: "Mark Scheuerell"
date: "10/31/2019"
output:
  pdf_document:
    highlight: haddock
fontsize: 11pt
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "pdf")

if(!require("captioner")) {
  devtools::install_github("adletaw/captioner")
  library("captioner")
}

## set default caption delimter
fig_cap <- captioner(suffix = ".")
## initialize figure number
cnt <- 1
```


# Comparison of existing and reference scenarios

The focus of the modeling analysis is a comparison of results obtained with two scenarios: a "reference" case that represents a system without anthropogenic inputs, and an "existing" case that represents contemporary conditions. Specifically, Ecology is interested in the difference between the modeled concentration of dissolved oxygen estimated via the two models. In addition, Ecology would like to know the estimated uncertainty in that difference.

## Error in assumptions

In the section titled "Uncertainty in Dissolved Oxygen Depletion Estimates" (p59), it states,

> The RMSE of differences is calculated to understand the uncertainty associated with the result of subtracting one model scenario from another model scenario (i.e., the difference between two model scenarios). In this case, we calculated the error associated with the DO depletions computed from the difference between the existing and reference model scenarios.


there is an error in the assumed relationship between the standard deviation of the predictions and the root mean squared error (RMSE) of the predictions. Specifically, the document incorrectly suggests that

$$
\text{Var}(\hat{y}) = \text{RMSE}(\hat{y})^2 ~ \Rightarrow ~ \text{SD}(\hat{y}) = \text{RMSE}(\hat{y}).
$$

Specifically, the standard deviation of ${\hat{y}}$ is based upon differences between the predicted data points $\hat{y}_i$ and their mean $m_{\hat{y}}$

$$
\text{SD}(\hat{y}) = \sqrt{\frac{(\hat{y}_i - m_{\hat{y}})^2}{N}},
$$

whereas the RMSE of $\hat{y}$ is based upon differences between the predicted data points and their corresponding observed values $x_i$

$$
\text{RMSE}(\hat{y}) = \sqrt{\frac{(\hat{y}_i - y_i)^2}{N}}.
$$

Thus,

$$
\begin{aligned}
\sqrt{\frac{(\hat{y}_i - m_{\hat{y}})^2}{N}} &\neq \sqrt{\frac{(\hat{y}_i - y_i)^2}{N}} \\
\text{SD}(\hat{y}) &\neq \text{RMSE}(\hat{y}).
\end{aligned}
$$

\vspace{0.25in}

```{r regr_ex, echo = FALSE}
## set random seed for reprodicibility
set.seed(123)

## function to calculate RMSE
rmse <- function(pre, obs) {
  rmse <- sqrt(sum((pre - obs)^2)/length(pre))
  return(rmse)
}

## sample size
nn <- 20
## some values for the predictor x
xx <- sort(runif(nn, 0, 20))
## intercept
beta_0 <- 1
## slope
beta_1 <- 0.5
## random errors (mean = 0, SD = 1)
epsilon <- rnorm(nn)
## observed data
yy <- beta_0 + beta_1*xx + epsilon

## fit a regression model
mm <- lm(yy ~ xx)
## get predicted values
yhat <- predict(mm)
```

# An example

Here is a simple example that shows how $\text{SD}(\hat{y})$ and $\text{RMSE}(\hat{y})$ are different. Consider a case where we had reason to believe that a variable $y$ was a function of another variable $x$. In effort to undercover the nature of their relationship, we collected 20 samples of both $y$ and $x$ (Figure 1).

```{r fig_1_data, echo = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1,
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Plot of some hypothetical data.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}

Based on the apparent relationship between $x$ and $y$, we might assume that each of the observed values $y_i$ is a linear combination of an intercept $\beta_0$, the effect $\beta_1$ of a covariate $x_i$, and some random observation error $\epsilon_i$, such that

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i,
$$

and $\epsilon_i \sim \text{N}(0, \sigma)$. We could easily estimate the unknown parameters in this model ($\beta_0$, $\beta_1$, $\sigma$), and then use the deterministic portion of the model to make predictions to compare with each of the observed values. Specifically, the predictions $(\hat{y}_i)$ would be given by a straight line, such that

$$
\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i.
$$

We could then estimate the SD and RMSE of these predictions (Figure 2). It turns out that the SD of $\hat{y}$ is ~`r round(sd(yhat), 2)`, but the RMSE is only ~`r round(rmse(yhat, yy), 2)`, which is about `r round(sd(yhat) / rmse(yhat, yy), 0)` times less.


```{r fig_2_plot_regr_ex, echo = FALSE, fig.width = 8, fig.height = 4.5, fig.align="center"}
par(mfrow = c(1,2), mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 16, las = 1, type = "n",
     xlab = expression(italic(x)),
     ylab = expression(hat(italic(y))~~or~~m[hat(italic(y))]))
mtext("SD", side = 3, line = 1)
segments(x0 = xx, y0 = yhat, y1 = mean(yhat), col = "gray")
points(xx, yhat, pch = 21, bg = "white")
points(xx, rep(mean(yy), nn), pch = 19)
## plot observed and yhat
plot(xx, yy, pch = 16, las = 1, type = "n",
     xlab = expression(italic(x)),
     ylab = expression(italic(y)~~or~~hat(italic(y))))
mtext("RMSE", side = 3, line = 1)
segments(x0 = xx, y0 = yhat, y1 = yy, col = "gray")
points(xx, yy, pch = 19)
points(xx, yhat, pch = 21, bg = "white")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Graphical examples of the difference between the SD of the predictions (left) and the RMSE of the predictions (right). For the SD, the comparison is based upon the differences between the predictions (open circles) and their mean (filled circles). For the RMSE, the comparison is based upon differences between the predictions (open circles) and the observed data (filled circles). In both cases, one would square the length of each of the vertical gray lines, sum them up, and divide by the number of them before finally taking the square root.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.25in}

# Prediction errors

The above example dismisses an important aspect of RMSE: it should be used to compare "out of sample" predictions. Furthermore, RMSE give us an indication as to the predictive error, *on average*, rather than the uncertainty in a specific prediction.

Returning to our example above, we could estimate our uncertainty around the fitted relationship between $x$ and $y$ with a confidence interval (CI), which would give us an indication of the range of where the "true" fitted values would lie had we repeated our sampling exercise many times. Specifically, a $(1 - \alpha)100$% CI on the expected relationship between $x$ and $y$ at some value $x_k$ is given by

$$
\hat{y}_{i} \pm t_{\alpha / 2, n-2} \sqrt{\sigma \left(\frac{1}{n}+\frac{\left(x_{k}-\bar{x}\right)^{2}}{\sum\left(x_{i}-\bar{x}\right)^{2}}\right)}.
$$

The interval increases as the distance between $x_k$ and $\bar{x}$ increases (Figure 3).

```{r fig_3_CI, echo = FALSE, warning = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
y_ci <- predict(mm, interval = "confidence")
y_pi <- predict(mm, interval = "prediction")

par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1, ylim = range(y_pi),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
lines(xx, y_ci[,"fit"])
lines(xx, y_ci[,"lwr"], col = "gray")
lines(xx, y_ci[,"upr"], col = "gray")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Example of a 95% confidence interval (gray lines) around the expected relationship between $x$ and $y$ (black line).", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}

In a case like this, however, where we wish to make out-of-sample predictions about some new state of nature, our uncertainty around any single prediction will be necessarily greater. Specifically, a $(1 - \alpha)100$% prediction interval (PI) around $\hat{y}$ at some value $x_k$ is given by

$$
\hat{y} \pm t_{\alpha / 2, n-2} \sqrt{\sigma \left(1 + \frac{1}{n}+\frac{\left(x_{k}-\bar{x}\right)^{2}}{\sum\left(x_{i}-\bar{x}\right)^{2}}\right)}.
$$

```{r y_new, echo = FALSE}
y_new <- predict(mm, new = data.frame(xx = 10), interval = "prediction")
```

Here the paranthetic multiplier on the residual variance $\sigma$ has increased by 1, which means the prediction interval is wider (less certain) than the confidence interval (Figure 4). This is because the CI only needs to account for uncertainty in estimating the expected value of $y$ whereas the PI needs to account for a random future value of $y$ that tend to fall away from the mean.

```{r fig_4_PI, echo = FALSE, warning = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1, ylim = range(y_pi),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
lines(xx, y_pi[,"fit"])
lines(xx, y_pi[,"lwr"], col = "gray")
lines(xx, y_pi[,"upr"], col = "gray")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Example of a 95% prediction interval (gray lines) for future unobserved values of $y$.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}

So, for example, if we wanted to predict, with 95% certainty, what we would observe for $y$ if $x = 10$, we would get `r round(y_new[,"fit"], 2)` $\pm$ `r round(y_new[,"fit"] - y_new[,"lwr"], 2)` (Figure 5). The relatively wide prediction interval suggests that it might be difficult to discern the prediction for $y$ when $x = 10$ to the expected values for $y$ if $x$ were as low as 5 or as high as 15.

\vspace{0.2in}

```{r fig_5_new_y, echo = FALSE, warning = FALSE, fig.height=4.5, fig.width=4, fig.pos="placeHere", fig.align="center"}
par(mai = c(1.5,1,0.5,0.25), cex = 0.7)
## plot yhat and mean(yhat)
plot(xx, yy, pch = 19, las = 1, ylim = range(y_pi),
     xlab = expression(italic(x)),
     ylab = expression(italic(y)))
lines(xx, y_pi[,"fit"])
abline(h = y_new[,"lwr"], col = "gray")
abline(h = y_new[,"upr"], col = "gray")
segments(10, y0 = y_new[,"lwr"], y1 = y_new[,"upr"], col = "blue")
points(10, y_new[,"fit"], pch = 19, col = "blue")
```

```{r, echo = FALSE}
cnt <- cnt + 1
```

\setlength{\leftskip}{0.375in}
\setlength{\rightskip}{0.375in}

\small

`r fig_cap(cnt, caption = "Example of the uncertainty around a new prediction for $y$ when $x = 10$.", display = "full")` 

\normalsize

\setlength{\leftskip}{0in}
\setlength{\rightskip}{0in}

\vspace{0.2in}




